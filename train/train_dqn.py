import torch
import pandas as pd
from agent.dqn_agent import DQNAgent
from env.forex_env import ForexEnv

# === è·¯å¾„é…ç½® ===
DATA_PATH = "/Users/junxuanzhang/Desktop/agent/data/processed/train_state.csv"
MODEL_PATH = "/Users/junxuanzhang/Desktop/agent/logs/dqn_model.pth"
LOG_PATH = "/Users/junxuanzhang/Desktop/agent/logs/training_log.csv"
PLOT_PATH = "/Users/junxuanzhang/Desktop/agent/logs/training_plot.png"
DEVICE = "cpu"

# === ç¯å¢ƒä¸æ¨¡å‹åˆå§‹åŒ– ===
env = ForexEnv(DATA_PATH, fee=0.0015)
state_dim = 6  # CPI, InterestRate, Unemployment, tone, exchange_rate, holding
action_dim = 3  # Hold, Buy, Sell
agent = DQNAgent(state_dim, action_dim, device=DEVICE)

# === è®­ç»ƒè¿‡ç¨‹ ===
EPISODES = 300
TARGET_UPDATE_FREQ = 10

for episode in range(EPISODES):
    state = env.reset()  # é‡ç½®ç¯å¢ƒ
    total_reward = 0
    done = False
    while not done:
        action = agent.select_action(state)  # æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
        next_state, reward, done, profit = env.step(action)  # è·å–å››ä¸ªå€¼ï¼Œæ³¨æ„å¢åŠ äº† profit
        agent.remember(state, action, reward, next_state, done)  # è®°ä½å½“å‰çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±
        agent.learn()  # å­¦ä¹ æ›´æ–° Q ç½‘ç»œ
        state = next_state
        total_reward += reward  # ç´¯è®¡å¥–åŠ±

    # æ¯ N è½®åŒæ­¥ç›®æ ‡ç½‘ç»œ
    if episode % TARGET_UPDATE_FREQ == 0:
        agent.update_target()
    # epsilon è¡°å‡
    if agent.epsilon > agent.epsilon_min:
        agent.epsilon *= agent.epsilon_decay

    print(f"âœ… Episode {episode+1}/{EPISODES} - Total reward: {total_reward:.4f} - Epsilon: {agent.epsilon:.4f}")

# === ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ ===
torch.save(agent.q_net.state_dict(), MODEL_PATH)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜")